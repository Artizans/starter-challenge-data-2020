{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-12T10:37:45.572535Z",
     "start_time": "2019-11-12T10:37:44.174630Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, UpSampling2D\n",
    "from tensorflow.keras.layers import Input, add, concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "import threading\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.python.keras.utils.data_utils import Sequence\n",
    "\n",
    "from losses import (\n",
    "    binary_crossentropy,\n",
    "    dice_loss,\n",
    "    bce_dice_loss,\n",
    "    dice_coef,\n",
    "    weighted_bce_dice_loss\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-12T10:37:45.584166Z",
     "start_time": "2019-11-12T10:37:45.574148Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def encoder(x, filters=44, n_block=3, kernel_size=(3, 3), activation='relu'):\n",
    "    skip = []\n",
    "    for i in range(n_block):\n",
    "        x = Conv2D(filters * 2**i, kernel_size, activation=activation, padding='same')(x)\n",
    "        x = Conv2D(filters * 2**i, kernel_size, activation=activation, padding='same')(x)\n",
    "        skip.append(x)\n",
    "        x = MaxPooling2D(pool_size=(2, 2), strides=(2, 2))(x)\n",
    "    return x, skip\n",
    "\n",
    "\n",
    "def bottleneck(x, filters_bottleneck, mode='cascade', depth=6,\n",
    "               kernel_size=(3, 3), activation='relu'):\n",
    "    dilated_layers = []\n",
    "    if mode == 'cascade':  # used in the competition\n",
    "        for i in range(depth):\n",
    "            x = Conv2D(filters_bottleneck, kernel_size,\n",
    "                       activation=activation, padding='same', dilation_rate=2**i)(x)\n",
    "            dilated_layers.append(x)\n",
    "        return add(dilated_layers)\n",
    "    elif mode == 'parallel':  # Like \"Atrous Spatial Pyramid Pooling\"\n",
    "        for i in range(depth):\n",
    "            dilated_layers.append(\n",
    "                Conv2D(filters_bottleneck, kernel_size,\n",
    "                       activation=activation, padding='same', dilation_rate=2**i)(x)\n",
    "            )\n",
    "        return add(dilated_layers)\n",
    "\n",
    "\n",
    "def decoder(x, skip, filters, n_block=3, kernel_size=(3, 3), activation='relu'):\n",
    "    for i in reversed(range(n_block)):\n",
    "        x = UpSampling2D(size=(2, 2))(x)\n",
    "        x = Conv2D(filters * 2**i, kernel_size, activation=activation, padding='same')(x)\n",
    "        x = concatenate([skip[i], x])\n",
    "        x = Conv2D(filters * 2**i, kernel_size, activation=activation, padding='same')(x)\n",
    "        x = Conv2D(filters * 2**i, kernel_size, activation=activation, padding='same')(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def get_dilated_unet(\n",
    "        input_shape=(1920, 1280, 3),\n",
    "        mode='cascade',\n",
    "        filters=44,\n",
    "        n_block=3,\n",
    "        lr=0.0001,\n",
    "        loss=bce_dice_loss,\n",
    "        n_class=1\n",
    "):\n",
    "    inputs = Input(input_shape)\n",
    "    \n",
    "    enc, skip = encoder(inputs, filters, n_block)\n",
    "    bottle = bottleneck(enc, filters_bottleneck=filters * 2**n_block, mode=mode)\n",
    "    dec = decoder(bottle, skip, filters, n_block)\n",
    "    classify = Conv2D(n_class, (1, 1), activation='sigmoid')(dec)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=classify)\n",
    "    model.compile(optimizer=RMSprop(lr), loss=loss, metrics=[dice_coef])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-12T10:42:01.141415Z",
     "start_time": "2019-11-12T10:42:01.130623Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "WIDTH = 1280\n",
    "HEIGHT = 720\n",
    "BATCH_SIZE = 2\n",
    "\n",
    "def train_generator(id_list):\n",
    "    while True:\n",
    "        shuffle_indices = np.arange(len(id_list))\n",
    "        shuffle_indices = np.random.permutation(shuffle_indices)\n",
    "        \n",
    "        for start in range(0, len(id_list), BATCH_SIZE):\n",
    "            x_batch = []\n",
    "            y_batch = []\n",
    "            \n",
    "            end = min(start + BATCH_SIZE, len(id_list))\n",
    "            ids_batch = id_list[shuffle_indices[start:end]]\n",
    "            \n",
    "            for _id in ids_batch:\n",
    "                img = cv2.imread('input/images/{}'.format(_id))\n",
    "                mask = cv2.imread('input/masks/A_alpha_{}.png'.format(_id.replace('.jpg', '')), cv2.IMREAD_GRAYSCALE)\n",
    "                mask = np.expand_dims(mask, axis=-1)\n",
    "                assert mask.ndim == 3\n",
    "                \n",
    "                # === You can add data augmentations here. === #\n",
    "                if np.random.random() < 0.5:\n",
    "                    img, mask = img[:, ::-1, :], mask[..., ::-1, :]  # random horizontal flip\n",
    "                \n",
    "                x_batch.append(img)\n",
    "                y_batch.append(mask)\n",
    "            \n",
    "            x_batch = np.array(x_batch, np.float32) / 255.\n",
    "            y_batch = np.array(y_batch, np.float32) / 255.\n",
    "            \n",
    "            yield x_batch, y_batch\n",
    "\n",
    "\n",
    "def valid_generator(id_list):\n",
    "    while True:\n",
    "        for start in range(0, len(id_list), BATCH_SIZE):\n",
    "            x_batch = []\n",
    "            y_batch = []\n",
    "\n",
    "            end = min(start + BATCH_SIZE, len(id_list))\n",
    "            ids_batch = id_list[start:end]\n",
    "            for _id in ids_batch:\n",
    "                img = cv2.imread('input/images/{}'.format(_id))\n",
    "                mask = cv2.imread('input/masks/{}.png'.format(_id.replace('.jpg', '')), cv2.IMREAD_GRAYSCALE)\n",
    "                mask = np.expand_dims(mask, axis=-1)\n",
    "                \n",
    "                assert mask.ndim == 3\n",
    "    \n",
    "                x_batch.append(img)\n",
    "                y_batch.append(mask)\n",
    "            \n",
    "            x_batch = np.array(x_batch, np.float32) / 255.\n",
    "            y_batch = np.array(y_batch, np.float32) / 255.\n",
    "            \n",
    "            yield x_batch, y_batch\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-12T10:42:01.746498Z",
     "start_time": "2019-11-12T10:42:01.741502Z"
    }
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-12T15:04:00.384741Z",
     "start_time": "2019-11-12T10:48:37.820743Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1112 11:48:38.189123 140515320526592 callbacks.py:1791] `epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "405/405 [==============================] - 248s 612ms/step - loss: 1.0566 - dice_coef: 0.4205 - val_loss: 0.8472 - val_dice_coef: 0.6367\n",
      "Epoch 2/100\n",
      "405/405 [==============================] - 242s 597ms/step - loss: 0.8393 - dice_coef: 0.6415 - val_loss: 0.7331 - val_dice_coef: 0.7059\n",
      "Epoch 3/100\n",
      "405/405 [==============================] - 242s 597ms/step - loss: 0.7915 - dice_coef: 0.6659 - val_loss: 0.6594 - val_dice_coef: 0.7211\n",
      "Epoch 4/100\n",
      "405/405 [==============================] - 242s 597ms/step - loss: 0.7438 - dice_coef: 0.6937 - val_loss: 0.6828 - val_dice_coef: 0.7308\n",
      "Epoch 5/100\n",
      "405/405 [==============================] - 242s 597ms/step - loss: 0.6924 - dice_coef: 0.7227 - val_loss: 0.6211 - val_dice_coef: 0.7470\n",
      "Epoch 6/100\n",
      "405/405 [==============================] - 242s 597ms/step - loss: 0.6621 - dice_coef: 0.7438 - val_loss: 0.6715 - val_dice_coef: 0.7560\n",
      "Epoch 7/100\n",
      "405/405 [==============================] - 242s 597ms/step - loss: 0.6104 - dice_coef: 0.7664 - val_loss: 0.5948 - val_dice_coef: 0.7778\n",
      "Epoch 8/100\n",
      "405/405 [==============================] - 242s 597ms/step - loss: 0.5860 - dice_coef: 0.7792 - val_loss: 0.5435 - val_dice_coef: 0.7812\n",
      "Epoch 9/100\n",
      "405/405 [==============================] - 242s 597ms/step - loss: 0.5621 - dice_coef: 0.7931 - val_loss: 0.6882 - val_dice_coef: 0.8030\n",
      "Epoch 10/100\n",
      "405/405 [==============================] - 242s 597ms/step - loss: 0.5331 - dice_coef: 0.8071 - val_loss: 0.5082 - val_dice_coef: 0.8096\n",
      "Epoch 11/100\n",
      "405/405 [==============================] - 242s 597ms/step - loss: 0.5220 - dice_coef: 0.8174 - val_loss: 0.4745 - val_dice_coef: 0.8138\n",
      "Epoch 12/100\n",
      "405/405 [==============================] - 242s 597ms/step - loss: 0.4930 - dice_coef: 0.8291 - val_loss: 0.5054 - val_dice_coef: 0.8048\n",
      "Epoch 13/100\n",
      "405/405 [==============================] - 242s 597ms/step - loss: 0.4745 - dice_coef: 0.8319 - val_loss: 0.4266 - val_dice_coef: 0.8368\n",
      "Epoch 14/100\n",
      "405/405 [==============================] - 242s 597ms/step - loss: 0.4806 - dice_coef: 0.8369 - val_loss: 0.6011 - val_dice_coef: 0.8177\n",
      "Epoch 15/100\n",
      "405/405 [==============================] - 242s 597ms/step - loss: 0.4589 - dice_coef: 0.8381 - val_loss: 0.5811 - val_dice_coef: 0.8262\n",
      "Epoch 16/100\n",
      "405/405 [==============================] - 242s 597ms/step - loss: 0.4830 - dice_coef: 0.8370 - val_loss: 0.4256 - val_dice_coef: 0.8484\n",
      "Epoch 17/100\n",
      "405/405 [==============================] - 242s 597ms/step - loss: 0.4527 - dice_coef: 0.8488 - val_loss: 0.5054 - val_dice_coef: 0.8018\n",
      "Epoch 18/100\n",
      "405/405 [==============================] - 242s 597ms/step - loss: 0.4628 - dice_coef: 0.8452 - val_loss: 0.4508 - val_dice_coef: 0.8454\n",
      "Epoch 19/100\n",
      "405/405 [==============================] - 242s 597ms/step - loss: 0.4372 - dice_coef: 0.8461 - val_loss: 0.4623 - val_dice_coef: 0.8399\n",
      "Epoch 20/100\n",
      "405/405 [==============================] - 242s 597ms/step - loss: 0.4251 - dice_coef: 0.8537 - val_loss: 0.6654 - val_dice_coef: 0.7577\n",
      "Epoch 21/100\n",
      "404/405 [============================>.] - ETA: 0s - loss: 0.4203 - dice_coef: 0.8549\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 1.9999999494757503e-05.\n",
      "405/405 [==============================] - 242s 598ms/step - loss: 0.4203 - dice_coef: 0.8550 - val_loss: 0.4588 - val_dice_coef: 0.8445\n",
      "Epoch 22/100\n",
      "405/405 [==============================] - 242s 597ms/step - loss: 0.3078 - dice_coef: 0.8833 - val_loss: 0.4150 - val_dice_coef: 0.8617\n",
      "Epoch 23/100\n",
      "405/405 [==============================] - 242s 598ms/step - loss: 0.2812 - dice_coef: 0.8905 - val_loss: 0.3869 - val_dice_coef: 0.8637\n",
      "Epoch 24/100\n",
      "405/405 [==============================] - 242s 597ms/step - loss: 0.2732 - dice_coef: 0.8937 - val_loss: 0.3657 - val_dice_coef: 0.8711\n",
      "Epoch 25/100\n",
      "405/405 [==============================] - 242s 598ms/step - loss: 0.2504 - dice_coef: 0.8999 - val_loss: 0.3463 - val_dice_coef: 0.8802\n",
      "Epoch 26/100\n",
      "405/405 [==============================] - 242s 597ms/step - loss: 0.2209 - dice_coef: 0.9123 - val_loss: 0.4225 - val_dice_coef: 0.8717\n",
      "Epoch 27/100\n",
      "405/405 [==============================] - 242s 597ms/step - loss: 0.2121 - dice_coef: 0.9172 - val_loss: 0.4433 - val_dice_coef: 0.8733\n",
      "Epoch 28/100\n",
      "405/405 [==============================] - 243s 601ms/step - loss: 0.1958 - dice_coef: 0.9254 - val_loss: 0.6527 - val_dice_coef: 0.8605\n",
      "Epoch 29/100\n",
      "405/405 [==============================] - 246s 607ms/step - loss: 0.1899 - dice_coef: 0.9283 - val_loss: 0.2767 - val_dice_coef: 0.9028\n",
      "Epoch 30/100\n",
      "405/405 [==============================] - 242s 599ms/step - loss: 0.1602 - dice_coef: 0.9351 - val_loss: 0.3797 - val_dice_coef: 0.8854\n",
      "Epoch 31/100\n",
      "405/405 [==============================] - 242s 597ms/step - loss: 0.1475 - dice_coef: 0.9409 - val_loss: 0.3067 - val_dice_coef: 0.8993\n",
      "Epoch 32/100\n",
      "405/405 [==============================] - 242s 597ms/step - loss: 0.1529 - dice_coef: 0.9401 - val_loss: 0.3012 - val_dice_coef: 0.9066\n",
      "Epoch 33/100\n",
      "405/405 [==============================] - 242s 597ms/step - loss: 0.1482 - dice_coef: 0.9409 - val_loss: 0.2737 - val_dice_coef: 0.9130\n",
      "Epoch 34/100\n",
      "405/405 [==============================] - 242s 598ms/step - loss: 0.1322 - dice_coef: 0.9477 - val_loss: 0.2600 - val_dice_coef: 0.9155\n",
      "Epoch 35/100\n",
      "405/405 [==============================] - 242s 597ms/step - loss: 0.1470 - dice_coef: 0.9438 - val_loss: 0.2665 - val_dice_coef: 0.9100\n",
      "Epoch 36/100\n",
      "405/405 [==============================] - 242s 597ms/step - loss: 0.1167 - dice_coef: 0.9501 - val_loss: 0.2915 - val_dice_coef: 0.9115\n",
      "Epoch 37/100\n",
      "405/405 [==============================] - 242s 599ms/step - loss: 0.1290 - dice_coef: 0.9495 - val_loss: 0.2791 - val_dice_coef: 0.9182\n",
      "Epoch 38/100\n",
      "405/405 [==============================] - 242s 598ms/step - loss: 0.1257 - dice_coef: 0.9513 - val_loss: 0.2797 - val_dice_coef: 0.9192\n",
      "Epoch 39/100\n",
      "405/405 [==============================] - 245s 604ms/step - loss: 0.1160 - dice_coef: 0.9547 - val_loss: 0.3830 - val_dice_coef: 0.9038\n",
      "Epoch 40/100\n",
      "405/405 [==============================] - 242s 597ms/step - loss: 0.1068 - dice_coef: 0.9562 - val_loss: 0.2928 - val_dice_coef: 0.9119\n",
      "Epoch 41/100\n",
      "405/405 [==============================] - 242s 597ms/step - loss: 0.1184 - dice_coef: 0.9534 - val_loss: 0.2963 - val_dice_coef: 0.9208\n",
      "Epoch 42/100\n",
      "405/405 [==============================] - 242s 597ms/step - loss: 0.1104 - dice_coef: 0.9573 - val_loss: 0.4284 - val_dice_coef: 0.8797\n",
      "Epoch 43/100\n",
      "405/405 [==============================] - 242s 597ms/step - loss: 0.1214 - dice_coef: 0.9549 - val_loss: 0.2972 - val_dice_coef: 0.9234\n",
      "Epoch 44/100\n",
      "405/405 [==============================] - 242s 597ms/step - loss: 0.1362 - dice_coef: 0.9538 - val_loss: 0.2909 - val_dice_coef: 0.9210\n",
      "Epoch 45/100\n",
      "405/405 [==============================] - 242s 597ms/step - loss: 0.0875 - dice_coef: 0.9624 - val_loss: 0.4924 - val_dice_coef: 0.9054\n",
      "Epoch 46/100\n",
      "405/405 [==============================] - 242s 597ms/step - loss: 0.0918 - dice_coef: 0.9661 - val_loss: 0.3216 - val_dice_coef: 0.9223\n",
      "Epoch 47/100\n",
      "405/405 [==============================] - 242s 597ms/step - loss: 0.0925 - dice_coef: 0.9634 - val_loss: 0.3189 - val_dice_coef: 0.9141\n",
      "Epoch 48/100\n",
      "404/405 [============================>.] - ETA: 0s - loss: 0.1229 - dice_coef: 0.9559\n",
      "Epoch 00048: ReduceLROnPlateau reducing learning rate to 3.999999898951501e-06.\n",
      "405/405 [==============================] - 242s 597ms/step - loss: 0.1232 - dice_coef: 0.9557 - val_loss: 0.2834 - val_dice_coef: 0.9221\n",
      "Epoch 49/100\n",
      "405/405 [==============================] - 243s 601ms/step - loss: 0.0522 - dice_coef: 0.9749 - val_loss: 0.3154 - val_dice_coef: 0.9246\n",
      "Epoch 50/100\n",
      "405/405 [==============================] - 245s 606ms/step - loss: 0.0474 - dice_coef: 0.9775 - val_loss: 0.3179 - val_dice_coef: 0.9268\n",
      "Epoch 51/100\n",
      "405/405 [==============================] - 245s 606ms/step - loss: 0.0490 - dice_coef: 0.9767 - val_loss: 0.3255 - val_dice_coef: 0.9230\n",
      "Epoch 52/100\n",
      "405/405 [==============================] - 246s 607ms/step - loss: 0.0501 - dice_coef: 0.9766 - val_loss: 0.3132 - val_dice_coef: 0.9274\n",
      "Epoch 53/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "405/405 [==============================] - 245s 606ms/step - loss: 0.0484 - dice_coef: 0.9770 - val_loss: 0.3323 - val_dice_coef: 0.9271\n",
      "Epoch 54/100\n",
      "405/405 [==============================] - 246s 606ms/step - loss: 0.0462 - dice_coef: 0.9781 - val_loss: 0.3390 - val_dice_coef: 0.9265\n",
      "Epoch 55/100\n",
      "405/405 [==============================] - 246s 606ms/step - loss: 0.0448 - dice_coef: 0.9783 - val_loss: 0.3890 - val_dice_coef: 0.9242\n",
      "Epoch 56/100\n",
      "405/405 [==============================] - 245s 606ms/step - loss: 0.0469 - dice_coef: 0.9774 - val_loss: 0.3814 - val_dice_coef: 0.9252\n",
      "Epoch 57/100\n",
      "405/405 [==============================] - 246s 607ms/step - loss: 0.0438 - dice_coef: 0.9785 - val_loss: 0.3421 - val_dice_coef: 0.9282\n",
      "Epoch 58/100\n",
      "405/405 [==============================] - 246s 606ms/step - loss: 0.0471 - dice_coef: 0.9782 - val_loss: 0.3556 - val_dice_coef: 0.9267\n",
      "Epoch 59/100\n",
      "405/405 [==============================] - 246s 606ms/step - loss: 0.0430 - dice_coef: 0.9788 - val_loss: 0.3706 - val_dice_coef: 0.9260\n",
      "Epoch 60/100\n",
      "405/405 [==============================] - 245s 606ms/step - loss: 0.0419 - dice_coef: 0.9796 - val_loss: 0.3624 - val_dice_coef: 0.9280\n",
      "Epoch 61/100\n",
      "405/405 [==============================] - 245s 606ms/step - loss: 0.0434 - dice_coef: 0.9792 - val_loss: 0.3809 - val_dice_coef: 0.9262\n",
      "Epoch 62/100\n",
      "404/405 [============================>.] - ETA: 0s - loss: 0.0424 - dice_coef: 0.9796\n",
      "Epoch 00062: ReduceLROnPlateau reducing learning rate to 7.999999979801942e-07.\n",
      "405/405 [==============================] - 246s 607ms/step - loss: 0.0424 - dice_coef: 0.9797 - val_loss: 0.3967 - val_dice_coef: 0.9242\n",
      "Epoch 63/100\n",
      "405/405 [==============================] - 245s 606ms/step - loss: 0.0386 - dice_coef: 0.9812 - val_loss: 0.3763 - val_dice_coef: 0.9274\n",
      "Epoch 64/100\n",
      " 25/405 [>.............................] - ETA: 3:39 - loss: 0.0490 - dice_coef: 0.9731"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-47de9aa9dc7a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m                     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalid_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mids_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m                     validation_steps=np.ceil(float(len(ids_valid)) / float(BATCH_SIZE)))\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/python_3/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1431\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1432\u001b[0m         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1433\u001b[0;31m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m   1434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1435\u001b[0m   def evaluate_generator(self,\n",
      "\u001b[0;32m~/anaconda3/envs/python_3/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m       \u001b[0mis_deferred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_compiled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m       \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python_3/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m   1173\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_sample_weight_modes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1174\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1175\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python_3/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3291\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3292\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3293\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3294\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[0;32m~/anaconda3/envs/python_3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with open('train_ids.txt') as f:\n",
    "    all_ids = [line.rstrip() for line in f.readlines()]\n",
    "\n",
    "ids_train, ids_valid = train_test_split(all_ids, test_size=0.1, random_state=42)\n",
    "ids_train, ids_valid = np.array(ids_train), np.array(ids_valid)\n",
    "\n",
    "model = get_dilated_unet(\n",
    "    input_shape=(HEIGHT, WIDTH, 3),\n",
    "    mode='cascade',\n",
    "    filters=32,\n",
    "    n_class=1\n",
    ")\n",
    "\n",
    "callbacks = [EarlyStopping(monitor='val_dice_coef',\n",
    "                           patience=10,\n",
    "                           verbose=1,\n",
    "                           min_delta=1e-4,\n",
    "                           mode='max'),\n",
    "             ReduceLROnPlateau(monitor='val_dice_coef',\n",
    "                               factor=0.2,\n",
    "                               patience=5,\n",
    "                               verbose=1,\n",
    "                               epsilon=1e-4,\n",
    "                               mode='max'),\n",
    "             ModelCheckpoint(monitor='val_dice_coef',\n",
    "                             filepath='model_weights.hdf5',\n",
    "                             save_best_only=True,\n",
    "                             mode='max')]\n",
    "\n",
    "model.fit_generator(generator=train_generator(ids_train),\n",
    "                    steps_per_epoch=np.ceil(float(len(ids_train)) / float(BATCH_SIZE)),\n",
    "                    epochs=100,\n",
    "                    verbose=1,\n",
    "                    callbacks=callbacks,\n",
    "                    validation_data=valid_generator(ids_valid),\n",
    "                    validation_steps=np.ceil(float(len(ids_valid)) / float(BATCH_SIZE)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_3",
   "language": "python",
   "name": "python_3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
